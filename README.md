# Pyspark

PySpark is a Python API for Apache Spark. Apache Spark is an analytical processing engine for large scale powerful distributed data processing and machine learning applications.
Spark basically written in Scala and later on due to its industry adaptation itâ€™s API PySpark released for Python using Py4J. Py4J is a Java library that is integrated within PySpark and allows python to dynamically interface with JVM objects, hence to run PySpark you also need Java to be installed along with Python, and Apache Spark.
PySpark has used a lot in the machine learning & Data scientists community; thanks to vast python machine learning libraries. Spark runs operations on billions and trillions of data on distributed clusters 100 times faster than the traditional python applications.

1. In-memory computation
2. Distributed processing using parallelize
3. Can be used with many cluster managers (Spark, Yarn, Mesos e.t.c)
4. Fault-tolerant
5. Immutable
6. Lazy evaluation
7. Cache & persistence
8. Inbuild-optimization when using DataFrames
9. Supports ANSI SQL


### ADVANTAGES OF PYSPARK

PySpark is a general-purpose, in-memory, distributed processing engine that allows you to process data efficiently in a distributed fashion.
Applications running on PySpark are 100x faster than traditional systems.
You will get great benefits using PySpark for data ingestion pipelines.
Using PySpark we can process data from Hadoop HDFS, AWS S3, and many file systems.
PySpark also is used to process real-time data using Streaming and Kafka.
Using PySpark streaming you can also stream files from the file system and also stream from the socket.
PySpark natively has machine learning and graph libraries.

