{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spread-likelihood",
   "metadata": {},
   "source": [
    "# Machine Learning Example with PySpark\n",
    "\n",
    "Now that you have a brief idea of Spark and SQLContext, you are ready to build your first Machine learning program.\n",
    "\n",
    "## Following are the steps to build a Machine Learning program with PySpark:\n",
    "\n",
    "Step 1) Basic operation with PySpark\n",
    "\n",
    "Step 2) Data preprocessing\n",
    "\n",
    "Step 3) Build a data processing pipeline\n",
    "\n",
    "Step 4) Build the classifier: logistic\n",
    "\n",
    "Step 5) Train and evaluate the model\n",
    "\n",
    "Step 6) Tune the hyperparameter\n",
    "\n",
    "In this PySpark Machine Learning tutorial, we will use the adult dataset. The purpose of this tutorial is to learn how to use Pyspark. For more information about the dataset, refer to this tutorial.\n",
    "\n",
    " Note that, the dataset is not significant and you may think that the computation takes a long time. Spark is designed to process a considerable amount of data. Spark's performances increase relative to other machine learning libraries when the dataset processed grows larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "continental-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pressing-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-click",
   "metadata": {},
   "source": [
    "## Step 1) Basic operation with PySpark\n",
    "First of all, you need to initialize the SQLContext is not already in initiated yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "healthy-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "url = \"hr_employee.csv\"\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-dodge",
   "metadata": {},
   "source": [
    "then, you can read the cvs file with sqlContext.read.csv. You use inferSchema set to True to tell Spark to guess automatically the type of data. By default, it is turn to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chubby-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get(\"hr_employee.csv\"),header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "active-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: integer (nullable = true)\n",
      " |-- satisfaction_level: double (nullable = true)\n",
      " |-- last_evaluation: double (nullable = true)\n",
      " |-- number_project: integer (nullable = true)\n",
      " |-- average_montly_hours: integer (nullable = true)\n",
      " |-- time_spend_company: integer (nullable = true)\n",
      " |-- Work_accident: integer (nullable = true)\n",
      " |-- promotion_last_5years: integer (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- left: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now let's take a view on inferSchema \n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-appliance",
   "metadata": {},
   "source": [
    "You can see the data with show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clean-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+---------------+--------------+--------------------+------------------+-------------+---------------------+------+----+\n",
      "|empid|satisfaction_level|last_evaluation|number_project|average_montly_hours|time_spend_company|Work_accident|promotion_last_5years|salary|left|\n",
      "+-----+------------------+---------------+--------------+--------------------+------------------+-------------+---------------------+------+----+\n",
      "|1    |0.38              |0.53           |2             |157                 |3                 |0            |0                    |low   |1   |\n",
      "|2    |0.8               |0.86           |5             |262                 |6                 |0            |0                    |medium|1   |\n",
      "|3    |0.11              |0.88           |7             |272                 |4                 |0            |0                    |medium|1   |\n",
      "|4    |0.72              |0.87           |5             |223                 |5                 |0            |0                    |low   |1   |\n",
      "|5    |0.37              |0.52           |2             |159                 |3                 |0            |0                    |low   |1   |\n",
      "+-----+------------------+---------------+--------------+--------------------+------------------+-------------+---------------------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-bottom",
   "metadata": {},
   "source": [
    "If you didn't set inderShema to True, here is what is happening to the type. There are all in string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "registered-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: string (nullable = true)\n",
      " |-- satisfaction_level: string (nullable = true)\n",
      " |-- last_evaluation: string (nullable = true)\n",
      " |-- number_project: string (nullable = true)\n",
      " |-- average_montly_hours: string (nullable = true)\n",
      " |-- time_spend_company: string (nullable = true)\n",
      " |-- Work_accident: string (nullable = true)\n",
      " |-- promotion_last_5years: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- left: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_string = sqlContext.read.csv(SparkFiles.get(\"hr_employee.csv\"),header=True, inferSchema = False)\n",
    "df_string.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-revelation",
   "metadata": {},
   "source": [
    "To convert the continuous variable in the right format, you can use recast the columns. You can use withColumn to tell Spark which column to operate the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affected-cooperation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: float (nullable = true)\n",
      " |-- satisfaction_level: string (nullable = true)\n",
      " |-- last_evaluation: string (nullable = true)\n",
      " |-- number_project: string (nullable = true)\n",
      " |-- average_montly_hours: string (nullable = true)\n",
      " |-- time_spend_company: string (nullable = true)\n",
      " |-- Work_accident: string (nullable = true)\n",
      " |-- promotion_last_5years: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- left: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Import all from 'sql.types'\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Write a custom function to convert the data type of Dataframe columns\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names:\n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "        return df\n",
    "\n",
    "#list of continuous features\n",
    "CONTI_FEATURES = ['empid','satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company','Work_accident','promotion_last_5years','salary','left']\n",
    "\n",
    "#Covert the type\n",
    "df_string = convertColumn(df_string, CONTI_FEATURES, FloatType())\n",
    "\n",
    "#Check the dataset\n",
    "df_string.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mineral-latest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: integer (nullable = true)\n",
      " |-- satisfaction_level: double (nullable = true)\n",
      " |-- last_evaluation: double (nullable = true)\n",
      " |-- number_project: integer (nullable = true)\n",
      " |-- average_montly_hours: integer (nullable = true)\n",
      " |-- time_spend_company: integer (nullable = true)\n",
      " |-- Work_accident: integer (nullable = true)\n",
      " |-- promotion_last_5years: integer (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- left: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "#stringIndexer = StringIndexer(inputCol=\"label\", outputCol=\"newlabel\")\n",
    "#model = StringIndexer.fit(df)\n",
    "#df = model.transform(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-mozambique",
   "metadata": {},
   "source": [
    "## Select columns\n",
    "\n",
    "You can select and show the rows with select and the names of the features. \n",
    "\n",
    "Below, empid and satisfaction_level are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "protecting-motor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|empid|satisfaction_level|\n",
      "+-----+------------------+\n",
      "|    1|              0.38|\n",
      "|    2|               0.8|\n",
      "|    3|              0.11|\n",
      "|    4|              0.72|\n",
      "|    5|              0.37|\n",
      "+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('empid','satisfaction_level').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-tonight",
   "metadata": {},
   "source": [
    "## Count by group\n",
    "\n",
    "If you want to count the number of occurence by group, you can chain:\n",
    "\n",
    "1. groupBy()\n",
    "\n",
    "2. count()\n",
    "\n",
    "together. In the PySpark example below, you count the number of rows by the time_spend_company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accomplished-glory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|time_spend_company|count|\n",
      "+------------------+-----+\n",
      "|                 8|  162|\n",
      "|                 7|  188|\n",
      "|                10|  214|\n",
      "|                 6|  718|\n",
      "|                 5| 1473|\n",
      "|                 4| 2557|\n",
      "|                 2| 3244|\n",
      "|                 3| 6443|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(\"time_spend_company\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-potter",
   "metadata": {},
   "source": [
    "## Describe the data\n",
    "\n",
    "To get a summary statistics, of the data, you can use describe(). It will compute the :\n",
    "\n",
    "1. count\n",
    "\n",
    "2. mean\n",
    "\n",
    "3. standarddeviation\n",
    "\n",
    "4. min\n",
    "\n",
    "5. max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "german-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+------------------+-------------------+------------------+--------------------+------------------+-------------------+---------------------+------+-------------------+\n",
      "|summary|           empid|satisfaction_level|    last_evaluation|    number_project|average_montly_hours|time_spend_company|      Work_accident|promotion_last_5years|salary|               left|\n",
      "+-------+----------------+------------------+-------------------+------------------+--------------------+------------------+-------------------+---------------------+------+-------------------+\n",
      "|  count|           14999|             14997|              14999|             14999|               14999|             14999|              14999|                14999| 14999|              14999|\n",
      "|   mean|          7500.0|0.6128625725145038| 0.7161017401159978|  3.80305353690246|   201.0503366891126| 3.498233215547703| 0.1446096406427095| 0.021268084538969265|  null| 0.2380825388359224|\n",
      "| stddev|4329.98267894919|0.2486344419832611|0.17116911062327556|1.2325923553183513|   49.94309937128406|1.4601362305354808|0.35171855238017957|   0.1442814645785825|  null|0.42592409938029885|\n",
      "|    min|               1|              0.09|               0.36|                 2|                  96|                 2|                  0|                    0|  high|                  0|\n",
      "|    max|           14999|               1.0|                1.0|                 7|                 310|                10|                  1|                    1|medium|                  1|\n",
      "+-------+----------------+------------------+-------------------+------------------+--------------------+------------------+-------------------+---------------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-stone",
   "metadata": {},
   "source": [
    "If you want the summary statistic of only one column, add the name of the column inside describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "featured-ghost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|    number_project|\n",
      "+-------+------------------+\n",
      "|  count|             14999|\n",
      "|   mean|  3.80305353690246|\n",
      "| stddev|1.2325923553183513|\n",
      "|    min|                 2|\n",
      "|    max|                 7|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('number_project').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-dress",
   "metadata": {},
   "source": [
    "## Crosstab computation\n",
    "In some occasion, it can be interesting to see the descriptive statistics between two pairwise columns. For instance, you can count the number of people with income below or above 50k by education level. This operation is called a crosstab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "closing-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.crosstab('salary','average_montly_hours').sort(\"salary_average_montly_hours\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-bangladesh",
   "metadata": {},
   "source": [
    "## Drop column\n",
    "There are two intuitive API to drop columns:\n",
    "\n",
    "1. drop(): Drop a column\n",
    "2. dropna(): Drop NA's\n",
    "\n",
    "Below you drop the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "discrete-peeing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['satisfaction_level',\n",
       " 'last_evaluation',\n",
       " 'number_project',\n",
       " 'average_montly_hours',\n",
       " 'time_spend_company',\n",
       " 'Work_accident',\n",
       " 'promotion_last_5years',\n",
       " 'salary',\n",
       " 'left']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop('empid').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-graduate",
   "metadata": {},
   "source": [
    "## Filter data\n",
    "You can use filter() to apply descriptive statistics in a subset of data. \n",
    "\n",
    "For instance, you can count the number of people number_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "leading-retreat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df.number_project < 40).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-fetish",
   "metadata": {},
   "source": [
    "## Descriptive statistics by group\n",
    "Finally, you can group data by group and compute statistical operations like the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dedicated-dominican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------------+\n",
      "|last_evaluation|avg(average_montly_hours)|\n",
      "+---------------+-------------------------+\n",
      "|           0.66|       194.07207207207207|\n",
      "|           0.84|       224.50680272108843|\n",
      "|           0.87|       224.03987730061348|\n",
      "|           0.93|       223.51301115241637|\n",
      "|           0.89|       220.09459459459458|\n",
      "|           0.39|        177.1153846153846|\n",
      "|           0.79|        218.9377593360996|\n",
      "|           0.72|       199.13270142180096|\n",
      "|            0.7|        197.4131455399061|\n",
      "|           0.54|       173.74857142857144|\n",
      "|           0.45|       157.66086956521738|\n",
      "|           0.61|       198.05555555555554|\n",
      "|           0.99|       215.01162790697674|\n",
      "|            1.0|       220.70671378091873|\n",
      "|            0.6|       196.89592760180994|\n",
      "|           0.64|       200.84255319148937|\n",
      "|            0.8|        222.6374501992032|\n",
      "|           0.63|       198.47457627118644|\n",
      "|           0.92|        222.4089219330855|\n",
      "|           0.42|       175.51785714285714|\n",
      "+---------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('last_evaluation').agg({'average_montly_hours': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-discount",
   "metadata": {},
   "source": [
    "## Step 2) Data preprocessing\n",
    "Data processing is a critical step in machine learning. After you remove garbage data, you get some important insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "otherwise-heavy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empid: integer (nullable = true)\n",
      " |-- satisfaction_level: double (nullable = true)\n",
      " |-- last_evaluation: double (nullable = true)\n",
      " |-- number_project: integer (nullable = true)\n",
      " |-- average_montly_hours: integer (nullable = true)\n",
      " |-- time_spend_company: integer (nullable = true)\n",
      " |-- Work_accident: integer (nullable = true)\n",
      " |-- promotion_last_5years: integer (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- left: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-reliance",
   "metadata": {},
   "source": [
    "### To view the column by row wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "structural-prefix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(empid=1, satisfaction_level=0.38, last_evaluation=0.53, number_project=2, average_montly_hours=157, time_spend_company=3, Work_accident=0, promotion_last_5years=0, salary='low', left=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COLUMNS = ['','','',............columns according to your arrangement]\n",
    "#df = df.select(COLUMNS)\n",
    "\n",
    "\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-furniture",
   "metadata": {},
   "source": [
    "## Step 3) Build a data processing pipeline\n",
    "Similar to scikit-learn, Pyspark has a pipeline API.\n",
    "\n",
    "A pipeline is very convenient to maintain the structure of the data. You push the data into the pipeline. Inside the pipeline, various operations are done, the output is used to feed the algorithm.\n",
    "\n",
    "For instance, one universal transformation in machine learning consists of converting a string to one hot encoder, i.e., one column by a group. One hot encoder is usually a matrix full of zeroes.\n",
    "\n",
    "The steps to transform the data are very similar to scikit-learn. You need to:\n",
    "\n",
    "1. Index the string to numeric\n",
    "2. Create the one hot encoder\n",
    "3. Transform the data\n",
    "\n",
    "Two APIs do the job: StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "\n",
    "#### 1. First of all, you select the string column to index. The inputCol is the name of the column in the dataset. outputCol is the new name given to the transformed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mature-mongolia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexer_8a17e52aaf52"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StringIndexer(inputCol = \"salary\", outputCol=\"salary_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-approach",
   "metadata": {},
   "source": [
    "### 2. fit the data and transform it\n",
    "\n",
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "protecting-devices",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "piano-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = StringIndexer.fit(df)\n",
    "#`indexers = model.transform(df)``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-indonesian",
   "metadata": {},
   "source": [
    "#### 3. Create the news columns based on the group. For instance, if there are 10 groups in the feature, the new matrix will have 10 columns, one for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "consistent-rapid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder_c3679b9e9732"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "OneHotEncoder(dropLast =False, inputCol=\"salaryencoded\", outputCol= \"salaryvec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-indonesian",
   "metadata": {},
   "source": [
    "## Build the pipeline\n",
    "You will build a pipeline to convert all the precise features and add them to the final dataset. The pipeline will have four operations, but feel free to add as many operations as you want.\n",
    "\n",
    "1. Encode the categorical data\n",
    "\n",
    "2. Index the label feature\n",
    "\n",
    "3. Add continuous variable\n",
    "\n",
    "4. Assemble the steps.\n",
    "\n",
    "Each step is stored in a list named stages. This list will tell the VectorAssembler what operation to perform inside the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-capacity",
   "metadata": {},
   "source": [
    "#### 1. Encode the categorical data\n",
    "\n",
    "This step is exaclty the same as the above example, except that you loop over all the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "accepting-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import pipeline\n",
    "CATE_FEATURES = ['salary']\n",
    "stages = [] #stages in our pipeline\n",
    "\n",
    "for categoricalCol in CATE_FEATURES:\n",
    "    stringIndexer = StringIndexer(inputCol= categoricalCol, outputCol= categoricalCol + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                    outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-solution",
   "metadata": {},
   "source": [
    "#### 2. Index the label feature\n",
    "\n",
    "Spark, like many other libraries, does not accept string values for the label. You convert the label feature with StringIndexer and add it to the list stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "unavailable-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using StringIndexer\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol =\"label\", outputCol = \"newlabel\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-queen",
   "metadata": {},
   "source": [
    "#### 3. Add continuous variable\n",
    "\n",
    "The inputCols of the VectorAssembler is a list of columns. You can create a new list containing all the new columns. The code below popluate the list with encoded categorical features and the continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sensitive-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-coast",
   "metadata": {},
   "source": [
    "#### 4. Assemble the steps.\n",
    "\n",
    "Finally, you pass all the steps in the VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "silver-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-parallel",
   "metadata": {},
   "source": [
    "Now that all the steps are ready, you push the data to the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "distant-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a pipeline\n",
    "\n",
    "#pipeline = Pipeline(stages = stages)\n",
    "#pipelineModel = pipeline.fit(df)\n",
    "#model = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-gather",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
