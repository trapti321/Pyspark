{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "noticed-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\python37\\lib\\site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 21.2.1 is available.\n",
      "You should consider upgrading via the 'c:\\python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abroad-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "other-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-vaccine",
   "metadata": {},
   "source": [
    "## Spark Context\n",
    "SparkContext is the internal engine that allows the connections with the clusters. If you want to run an operation, you need a SparkContext.\n",
    "\n",
    "## Create a SparkContext\n",
    "\n",
    "First of all, you need to initiate a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cordless-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-coalition",
   "metadata": {},
   "source": [
    "Now that the SparkContext is ready, you can create a collection of data called RDD, Resilient Distributed Dataset. \n",
    "\n",
    "Computation in an RDD is automatically parallelized across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "apparent-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-battery",
   "metadata": {},
   "source": [
    "You can access the first row with take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dominant-research",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-caribbean",
   "metadata": {},
   "source": [
    "You can apply a transformation to the data with a lambda function. In the PySpark example below, you return the square of nums. It is a map transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "built-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print('%i ' % (num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-harvey",
   "metadata": {},
   "source": [
    "## SQLContext\n",
    "A more convenient way is to use the DataFrame. SparkContext is already set, you can use it to create the dataFrame. You also need to declare the SQLContext\n",
    "\n",
    "SQLContext allows connecting the engine with different data sources. It is used to initiate the functionalities of Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabulous-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-resolution",
   "metadata": {},
   "source": [
    "Now in this Spark tutorial Python, let's create a list of tuple. Each tuple will contain the name of the people and their age. Four steps are required:\n",
    "\n",
    "Step 1) Create the list of tuple with the information\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "consistent-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_p = [('john',19),('siya',14),('rahul',35),('demon',50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-petite",
   "metadata": {},
   "source": [
    "Step 2) Build a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hollywood-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(list_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-understanding",
   "metadata": {},
   "source": [
    "Step 3) Convert the tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "removable-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-coating",
   "metadata": {},
   "source": [
    "Step 4) Create a DataFrame context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "weighted-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.createDataFrame(ppl)\n",
    "list_p = [('john',19),('siya',14),('rahul',35),('demon',50)]\n",
    "rdd = sc.parallelize(list_p)\n",
    "ppl= rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-dining",
   "metadata": {},
   "source": [
    "If you want to access the type of each feature,\n",
    "\n",
    "you can use printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "later-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-weather",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
